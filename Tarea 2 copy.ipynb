{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 Jafed Martinez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from random import choice\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import argparse\n",
    "\n",
    "from IMC.transformer import Transformer\n",
    "from train import train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando w2v en ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_w2v(w2v_path):\n",
    "    w2v = []\n",
    "    word_idx = {}\n",
    "    idx_word = []\n",
    "    with open(w2v_path, 'r') as file:\n",
    "        for n, line in enumerate(file):\n",
    "            if n==0:\n",
    "                vocab_len, w2v_dim = line.split(' ')\n",
    "            else:\n",
    "                line = line.split(' ')\n",
    "                word_idx[line[0]] = n-1\n",
    "                idx_word.append(line[0])\n",
    "                w2v.append(list(map(float, line[1:])))\n",
    "    return w2v, word_idx, idx_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v, word_idx, idx_word = read_w2v('word2vec_lim.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando las anotaciones tokenizadas y con una longitud maxima data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ims   = r'C:\\Users\\jafse\\Documents\\Maestria cimat\\Proyecto tecnologico\\YOLOv5-mask\\data\\coco2017\\train2017'\n",
    "val_ims     = r'C:\\Users\\jafse\\Documents\\Maestria cimat\\Proyecto tecnologico\\YOLOv5-mask\\data\\coco2017\\val2017'\n",
    "train_caps  = r'C:\\Users\\jafse\\Documents\\Maestria cimat\\Proyecto tecnologico\\YOLOv5-mask\\data\\coco2017\\annotations_2\\captions_train2017.json'\n",
    "val_caps    = r'C:\\Users\\jafse\\Documents\\Maestria cimat\\Proyecto tecnologico\\YOLOv5-mask\\data\\coco2017\\annotations_2\\captions_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions_to_w2v(captions_json, imgs_path, max_n_caption=5, max_cap_len=15, tk=TweetTokenizer(), word_idx=None):\n",
    "    '''\n",
    "    captions_json:  Path a las anotaciones\n",
    "    imgs_path:      Path a las imagenes\n",
    "    max_n_caption:  Maxima cantidad de captions por imagen\n",
    "    '''\n",
    "    ann_dict = {} \n",
    "    #Revisando que imagenes estan realmente\n",
    "    imgs = { img_file for img_file in os.listdir(imgs_path) }\n",
    "    \n",
    "    #revisando las captions\n",
    "    with open(captions_json) as file:\n",
    "        data = json.load(file)\n",
    "        for img_json in data[\"images\"]:\n",
    "            '''\n",
    "            Algunas imagenes estan en el json pero no en la carpeta por lo tanto \n",
    "            es mejor anotar las que si estan y las que no\n",
    "            '''\n",
    "            if img_json[\"file_name\"] in imgs: \n",
    "                ann_dict[img_json[\"id\"]] = { \n",
    "                    \"file_name\" : img_json[\"file_name\"]\n",
    "                 } \n",
    "                 \n",
    "        for cap_info in data[\"annotations\"]:\n",
    "            img_id = cap_info[\"image_id\"]\n",
    "            if img_id in ann_dict:  #Aqui igual asegurar que la imagen este en los 2 lugares\n",
    "                #Tokenizar caption\n",
    "                cap = tk.tokenize(cap_info[\"caption\"])\n",
    "\n",
    "                #Ajustar len de caption\n",
    "                if len(cap) < max_cap_len:\n",
    "                    cap += ['</s>']*(max_cap_len-len(cap))\n",
    "                elif len(cap) > max_cap_len:\n",
    "                    cap = cap[:max_cap_len]\n",
    "\n",
    "                if word_idx is not None:\n",
    "                    cap = [word_idx['<s>']] + [word_idx[word.lower()] if word.lower() in word_idx else 0 for word in cap  ]\n",
    "                \n",
    "                # Cada imagen tiene varios captions\n",
    "                if \"caption\" in ann_dict[img_id]:\n",
    "                    #Agregar caption\n",
    "                    if len(ann_dict[img_id][\"caption\"]) <  max_n_caption:\n",
    "                        ann_dict[img_id][\"caption\"].append(cap)\n",
    "                else:\n",
    "                    ann_dict[img_id][\"caption\"] =[cap]\n",
    "                    \n",
    "    return ann_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = load_captions_to_w2v(train_caps, train_ims, word_idx=word_idx)\n",
    "val_dict   = load_captions_to_w2v(val_caps, val_ims, word_idx=word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICDataset(Dataset):\n",
    "    def __init__(self, ann_dict, imgs_path, img_size=256):\n",
    "        '''\n",
    "        ann_dict:   Diccionario con las anotaciones ya tokenizadas y con embedding\n",
    "        imgs_path:\n",
    "        max_len:    Maxima longitud del caption\n",
    "        words_embedding_idx\n",
    "        '''\n",
    "        self.ann_dict   = ann_dict\n",
    "        self.imgs_path  = imgs_path\n",
    "\n",
    "        self.img_to_tensor = transforms.functional.pil_to_tensor\n",
    "        self.resize_img = transforms.Resize((img_size,img_size),antialias=True)\n",
    "\n",
    "        #Al leer los caption se guarda el id que no esta numerado por lo tanto creo un vector de idx\n",
    "        self.ann_idx = [x for x in self.ann_dict.keys()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_dict)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        #Cargando imagen\n",
    "        file_name = self.ann_dict[self.ann_idx[i]][\"file_name\"]\n",
    "        img = Image.open(self.imgs_path + \"/\" + file_name)\n",
    "        img = img.convert('RGB')#Algunas imagenes estan en blanco y negro\n",
    "        img = self.resize_img(self.img_to_tensor(img)).float()\n",
    "        \n",
    "        #Cargando captions\n",
    "        capts = self.ann_dict[self.ann_idx[i]][\"caption\"]\n",
    "        capts = torch.tensor(capts[0])\n",
    "        return img, capts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ICDataset(train_dict, train_ims)\n",
    "val_dataset = ICDataset(val_dict, val_ims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando los dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             drop_last=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "epochs = 5\n",
    "device = torch.device('cuda')\n",
    "weight_decay = 0.0001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "chkp_path = 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_len=len(word_idx), emb_dim=100, d_model=100, d_hidden=1028,\n",
    "                    n_layers=2, h=2, n_position=50, w2v=torch.tensor(w2v), mask=True)\n",
    "\n",
    "optimzer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas = (beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  36%|███▌      | 166/462 [03:54<07:35,  1.54s/it]"
     ]
    }
   ],
   "source": [
    "train(model, epochs, train_dataloader, val_dataloader, optimzer, device, chkp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
